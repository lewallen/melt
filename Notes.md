# Symmetries for deep network architectures

Writing down our comments on deep network symmetries.


# Intuition for the "wide minima = good generalization" heuristic

This is one of the main intuitions offered as an explanation of generalization in deep nets, and I'd like to understand it better, since it's not immediately clear to me why it should hold. Also, I imagine it will enter into any story related to the kind of ensembling we're interested in. Would love to get others' insights...

Abusing some notation, let <img src="/tex/f23990e4b614cdf57744cc78c2724e30.svg?invert_in_darkmode&sanitize=true" align=middle width=107.80972454999998pt height=24.65753399999998pt/> denote the training loss for the optimally trained weights <img src="/tex/7e5761f956ba40010cd6e62986fbf55c.svg?invert_in_darkmode&sanitize=true" align=middle width=68.72063385pt height=22.465723500000017pt/>, where <img src="/tex/84c95f91a742c9ceb460a83f9b5090bf.svg?invert_in_darkmode&sanitize=true" align=middle width=17.80826024999999pt height=22.465723500000017pt/> denotes the space of weights/parameters. I think the "wide minima heuristic" would be true *if* for every test set, we could always find some <img src="/tex/d296bb9b19425934b2ffccd914d86d9b.svg?invert_in_darkmode&sanitize=true" align=middle width=50.110244249999994pt height=22.465723500000017pt/> in the vicinity of <img src="/tex/db71b8514018f7f4533b3ab3922e2f39.svg?invert_in_darkmode&sanitize=true" align=middle width=29.99938094999999pt height=14.15524440000002pt/> (say <img src="/tex/229d508958ba23a5a46663eb4c5158df.svg?invert_in_darkmode&sanitize=true" align=middle width=135.38251484999998pt height=24.65753399999998pt/> for a small radius <img src="/tex/89f2e0d2d24bcf44db73aab8fc03252c.svg?invert_in_darkmode&sanitize=true" align=middle width=7.87295519999999pt height=14.15524440000002pt/>) such that

<p align="center"><img src="/tex/e0dd5b1203fcbec72ec1633c0752d9d6.svg?invert_in_darkmode&sanitize=true" align=middle width=209.6710374pt height=17.031940199999998pt/></p>

To be valid as a heuristic to learn about <img src="/tex/47291815667dfe5994c54805102e144b.svg?invert_in_darkmode&sanitize=true" align=middle width=11.337943649999989pt height=22.465723500000017pt/> I feel this needs to be true without any assumptions on <img src="/tex/47291815667dfe5994c54805102e144b.svg?invert_in_darkmode&sanitize=true" align=middle width=11.337943649999989pt height=22.465723500000017pt/>, so that we actually need <img src="/tex/31fae8b8b78ebe01cbfbe2fe53832624.svg?invert_in_darkmode&sanitize=true" align=middle width=12.210846449999991pt height=14.15524440000002pt/> to satisfy

<p align="center"><img src="/tex/48d9deb68ec233afe1d04022d85dfa91.svg?invert_in_darkmode&sanitize=true" align=middle width=193.38781275pt height=17.031940199999998pt/></p>

I can "kind of" see why this might be true: e.g. if the data were normally distributed and we could assume that <img src="/tex/9c6180eb4f3daf42155646793d0c2e07.svg?invert_in_darkmode&sanitize=true" align=middle width=119.58491819999999pt height=22.831056599999986pt/> with <img src="/tex/ef5f8e568bc89c138a5c7b7e417432d2.svg?invert_in_darkmode&sanitize=true" align=middle width=6.672392099999992pt height=14.15524440000002pt/> denoting a small normally distributed noise vector, and if...

Ok wait, stepping back: let <img src="/tex/cbfb1b2a33b28eab8a3e59464768e810.svg?invert_in_darkmode&sanitize=true" align=middle width=14.908688849999992pt height=22.465723500000017pt/> and <img src="/tex/91aac9730317276af725abd8cef04ca9.svg?invert_in_darkmode&sanitize=true" align=middle width=13.19638649999999pt height=22.465723500000017pt/> denote the input and output spaces for our regression problem, and think of the functions <img src="/tex/9ef670840b3e902d1dafbc4866c769d0.svg?invert_in_darkmode&sanitize=true" align=middle width=116.30133899999998pt height=24.65753399999998pt/> as embedded in <img src="/tex/aa46fb80cbf7d50b0d3510049d6760a3.svg?invert_in_darkmode&sanitize=true" align=middle width=48.196244249999985pt height=22.465723500000017pt/>, so we can think of our network architecture itself as a subset <img src="/tex/07476d36d4b76f1d6c97770825a42c01.svg?invert_in_darkmode&sanitize=true" align=middle width=133.92430919999998pt height=24.65753399999998pt/>. Therefore the tangent bundle <img src="/tex/a7e30f9fbd81d0480cea184f3e4e2f30.svg?invert_in_darkmode&sanitize=true" align=middle width=25.01486954999999pt height=22.465723500000017pt/> is naturally embedded in <img src="/tex/5f5da558137ec4e349a950e2c92845ad.svg?invert_in_darkmode&sanitize=true" align=middle width=266.34099855pt height=24.65753399999998pt/>. Then we want to try to rewrite equation <img src="/tex/d343a5beaabde2410ecf9f826344ed83.svg?invert_in_darkmode&sanitize=true" align=middle width=21.00464354999999pt height=24.65753399999998pt/> in terms of transversality of projections coming from <img src="/tex/4f7ac953e0d875228fe60821c709c517.svg?invert_in_darkmode&sanitize=true" align=middle width=23.50281614999999pt height=14.15524440000002pt/> and <img src="/tex/c72a872871f52b499d217293c895985e.svg?invert_in_darkmode&sanitize=true" align=middle width=21.045038849999987pt height=14.15524440000002pt/>, applied to <img src="/tex/a7e30f9fbd81d0480cea184f3e4e2f30.svg?invert_in_darkmode&sanitize=true" align=middle width=25.01486954999999pt height=22.465723500000017pt/>.

So <img src="/tex/2373cf469d437680d486880d09a60a97.svg?invert_in_darkmode&sanitize=true" align=middle width=18.721515449999988pt height=24.65753399999998pt/> am I overthinking things or <img src="/tex/9a7ba771198c084298aac5a69aa6a525.svg?invert_in_darkmode&sanitize=true" align=middle width=19.68612029999999pt height=24.65753399999998pt/> would it be interesting to think carefully about what these conditions really look like when the data is complex (eg, natural images)?
